# -*- coding: utf-8 -*-
"""Thesis Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1W3PsqUqXECdxBpUyYSglqZYuMc6Wejhj
"""

!pip install pysal
!pip install contextily

from pysal.lib import weights
from pysal.explore import esda
import numpy as np
import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt
import seaborn as sns
import contextily as ctx

# File paths
listings_path = '/content/listings.csv.gz'
attractions_path = '/content/austin_attractions.csv'
boroughs_path = '/content/atbb.shp'

# Reading the listings data
listings = pd.read_csv(listings_path, compression='gzip', low_memory=False)

"""**Data Cleaning and Preprocessing**"""

# Display columns of the dataset
print("Columns in the dataset:", listings.columns)

print(listings.head())

# Select relevant columns
listings_sub = listings[['id', 'neighbourhood_cleansed', 'beds', 'price', 'latitude', 'longitude',
                         'property_type', 'room_type', 'accommodates', 'bathrooms',
                         'bedrooms','review_scores_rating']]

listings_sub.info()

"""**Check for missing data**"""

# Check for missing data
print("Missing values before cleaning:")
print(listings_sub.isnull().sum())

# Drop rows with missing values in critical columns
listings_sub = listings_sub.dropna(subset=['price', 'beds', 'bedrooms', 'accommodates','bathrooms','review_scores_rating'])

# Again Checking for missing data
print("Missing values before cleaning:")
print(listings_sub.isnull().sum())

# Cleaning and converting the 'price' column
listings_sub.loc[:, "price"] = listings_sub["price"].replace("[$,]", "", regex=True).astype(float)

# Confirm conversion and check for any remaining non-numeric entries
print("Sample data after cleaning the 'price' column:")
print(listings_sub[['price']].head())

listings_sub.loc[:, 'beds'] = listings_sub['beds'].fillna(listings_sub['beds'].median())
listings_sub.loc[:, 'bathrooms'] = listings_sub['bathrooms'].fillna(listings_sub['bathrooms'].median())
listings_sub.loc[:, 'bedrooms'] = listings_sub['bedrooms'].fillna(
    listings_sub.groupby('neighbourhood_cleansed')['bedrooms'].transform('median')
)
listings_sub.loc[:, 'review_scores_rating'] = listings_sub['review_scores_rating'].fillna(
    listings_sub['review_scores_rating'].median()
)
listings_sub.dropna(subset=['price'], inplace=True)

# Again Confirm conversion and check for any remaining non-numeric entries
print("Sample data after cleaning the 'price' column:")
print(listings_sub[['price']].head())

# Again Checking for missing data
print("Missing values before cleaning:")
print(listings_sub.isnull().sum())

listings_sub.info()

listings_sub['price'] = listings_sub['price'].replace("[$,]", "", regex=True).astype(float)

print(listings_sub['price'].dtype)  # Should show float64

# Final check for missing values
print("Missing values after cleaning:")
print(listings_sub.isnull().sum())

"""**Exploratory Data Analysis & Processing**"""

# Display data types of each column to ensure they are appropriate
print("Data types after initial cleaning:")
print(listings_sub.dtypes)

# Describe the price variable for initial analysis
print("Price distribution summary:")
print(listings_sub["price"].describe())

# Plotting the distribution of Price variable
plt.figure(figsize=(10, 6))
sns.histplot(listings_sub["price"], kde=True, bins=30)
plt.title("Distribution of Price Variable")
plt.xlabel("Price (in USD)")
plt.ylabel("Frequency")
plt.show()

Q1 = listings_sub['price'].quantile(0.25)
Q3 = listings_sub['price'].quantile(0.75)
IQR = Q3 - Q1

upper_limit = Q3 + 2 * IQR  # Increase the threshold to retain more data
lower_limit = max(Q1 - 1.5 * IQR, listings_sub['price'].min())  # Avoid negative values

print(f"Upper Limit: {upper_limit}")
print(f"Lower Limit: {lower_limit}")

# Outlier Detection
Q1 = listings_sub['price'].quantile(0.25)
Q3 = listings_sub['price'].quantile(0.75)
IQR = Q3 - Q1

outliers = listings_sub[(listings_sub['price'] < (Q1 - 1.5 * IQR)) |
                        (listings_sub['price'] > (Q3 + 1.5 * IQR))]
print(f'Number of outliers: {len(outliers)}')

listings_sub = listings_sub[(listings_sub['price'] >= 8) &
                            (listings_sub['price'] <= 568)]

print(listings_sub['price'].describe())
sns.histplot(listings_sub['price'], bins=50, kde=True)
plt.title('Price Distribution After Outlier Removal')
plt.show()

print(f"Remaining rows after outlier removal: {len(listings_sub)}")

high_price_listings = listings_sub[listings_sub['price'] > 500]
sns.scatterplot(x='longitude', y='latitude', hue='price', data=high_price_listings, palette='coolwarm')

print(listings_sub.describe())

#Visualize the Distribution of Prices
import matplotlib.pyplot as plt

plt.figure(figsize=(8, 5))
listings_sub['price'].hist(bins=30)
plt.title('Distribution of Prices')
plt.xlabel('Price')
plt.ylabel('Frequency')
plt.show()

#Price By Neighbourhood
plt.figure(figsize=(12, 8))
sns.boxplot(x='neighbourhood_cleansed', y='price', data=listings_sub)
plt.xticks(rotation=90)
plt.title('Price Distribution by Neighborhood')
plt.show()

#Room type and price relationship
plt.figure(figsize=(8, 6))
sns.boxplot(x='room_type', y='price', data=listings_sub)
plt.title('Price by Room Type')
plt.show()

plt.figure(figsize=(10, 8))
sns.scatterplot(x='longitude', y='latitude', hue='price', palette='viridis', data=listings_sub)
plt.title('Geospatial Distribution of Listings by Price')
plt.show()

#Exploring how bedrooms and bathrooms influence price.
sns.boxplot(x='bedrooms', y='price', data=listings_sub)
plt.title('Price Distribution by Number of Bedrooms')
plt.show()

sns.boxplot(x='bathrooms', y='price', data=listings_sub)
plt.title('Price Distribution by Number of Bathrooms')
plt.show()

#Heatmap between price and features
plt.figure(figsize=(10, 6))
correlation_matrix = listings_sub[['price', 'beds', 'bathrooms', 'bedrooms', 'accommodates', 'review_scores_rating']].corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap of Numerical Features')
plt.show()

sns.pairplot(listings_sub[['price', 'beds', 'bathrooms', 'bedrooms', 'accommodates']])
plt.suptitle('Pairplot of Numerical Features', y=1.02)
plt.show()

plt.figure(figsize=(10, 6))
sns.countplot(x='room_type', data=listings_sub)
plt.title('Distribution of Room Types')
plt.xlabel('Room Type')
plt.ylabel('Count')
plt.show()

plt.figure(figsize=(10, 6))
sns.scatterplot(x='review_scores_rating', y='price', data=listings_sub)
plt.title('Review Scores vs. Price')
plt.xlabel('Review Scores Rating')
plt.ylabel('Price')
plt.show()

sns.scatterplot(x='review_scores_rating', y='price', hue='room_type', data=listings_sub)

sns.regplot(x='review_scores_rating', y='price', data=listings_sub, scatter_kws={'alpha':0.5})

# Convert listings to a GeoPandas DataFrame
listings_gpdf = gpd.GeoDataFrame(
    listings_sub,
    geometry=gpd.points_from_xy(listings_sub['longitude'], listings_sub['latitude']),
    crs="EPSG:4326"
)

# Reading the Austin borough shapefile
boroughs = gpd.read_file(boroughs_path)
# Projecting the boroughs to match listings' CRS if needed
boroughs = boroughs.to_crs("EPSG:4326")

# Reading the attractions data
attractions = pd.read_csv(attractions_path)
print("Attractions data columns:", attractions.columns)

# Convert attractions to GeoDataFrame
attractions_gpdf = gpd.GeoDataFrame(
    attractions,
    geometry=gpd.points_from_xy(attractions['Longitude'], attractions['Latitude']),
    crs="EPSG:4326"
)

# Filter listings to only include those within the Austin area
austin_boundary = boroughs
listings_austin_mask = listings_gpdf.within(austin_boundary.unary_union)
listings_austin = listings_gpdf.loc[listings_austin_mask]

# Plotting the listings and attractions on a basemap
fig, ax = plt.subplots(figsize=(10, 10))
austin_boundary.plot(ax=ax, color='lightgrey', edgecolor='black')
listings_austin.plot(ax=ax, color='blue', markersize=1, label='Listings')
attractions_gpdf.plot(ax=ax, color='red', markersize=5, label='Attractions')
ctx.add_basemap(ax, crs=listings_gpdf.crs.to_string())
plt.legend()
plt.show()

from shapely.ops import nearest_points

# Ensures CRS matches for distance calculations
attractions_gpdf = attractions_gpdf.to_crs(listings_gpdf.crs)

# Function to calculate distance to nearest attraction
def nearest_attraction_distance(listing_point, attractions_gpdf):
    nearest_geom = nearest_points(listing_point, attractions_gpdf.unary_union)[1]
    return listing_point.distance(nearest_geom)

# Calculate distance for each listing
listings_gpdf['distance_to_attraction'] = listings_gpdf['geometry'].apply(
    lambda x: nearest_attraction_distance(x, attractions_gpdf)
)

plt.figure(figsize=(10, 6))
sns.scatterplot(x='distance_to_attraction', y='price', data=listings_gpdf)
plt.title('Distance to Nearest Attraction vs. Price of Listings')
plt.xlabel('Distance to Nearest Attraction (degrees)')
plt.ylabel('Listing Price (USD)')
plt.show()

# Plot Distance vs. Price with Trend Line and Neighborhood Coloring
plt.figure(figsize=(12, 8))
sns.scatterplot(
    x='distance_to_attraction',
    y='price',
    hue='neighbourhood_cleansed',  # Color by neighborhood
    data=listings_gpdf,
    alpha=0.7  # Slight transparency for overlap
)

# Add trend line
sns.regplot(
    x='distance_to_attraction',
    y='price',
    data=listings_gpdf,
    scatter=False,
    color='black',  # Trend line color
    line_kws={'linewidth': 2, 'linestyle': '--'}  # Dotted trend line
)

plt.title('Distance to Nearest Attraction vs. Price of Listings (by Neighborhood)')
plt.xlabel('Distance to Nearest Attraction (meters)')
plt.ylabel('Listing Price (USD)')
plt.legend(title="Neighborhood", bbox_to_anchor=(1.05, 1), loc='upper left')  # Move legend outside plot
plt.show()

sns.scatterplot(x='distance_to_attraction', y='price', hue='room_type', data=listings_gpdf)

# Defining the variables of interest (VOI) to subset the data
voi = [
    'id',  # Unique identifier for the listing
    'room_type',  # Type of room
    'accommodates',  # The maximum capacity of the listing
    'bedrooms',  # The number of bedrooms
    'beds',  # The number of beds
    'review_scores_rating',  # The rating
    'price',  # The nightly rental rate, dependent variable (Y)
    #'rt_entire_home_apartment',
    #'rt_private_room',
    #'rt_shared_room'
]

# Ensures the selected columns exist in the Austin listings data
existing_columns = [col for col in voi if col in listings_austin.columns]
print("Existing columns in dataset:", existing_columns)

# Subsetting the Austin listings data
listings_austin_subset = listings_austin[voi]

listings_austin_subset.dtypes

# Display the first few rows of the subset
print(listings_austin_subset.head())

for col in listings_austin[['room_type']]:
    print(listings_austin[col].unique())

#One-hot encode the room_type variable
#For the room_type, all 0s represent 'Hotel room'
# listings_austin_subset.loc[:, 'rt_entire_home_apartment'] = np.where(listings_austin_subset["room_type"] == 'Entire home/apt', 1, 0)
# listings_austin_subset.loc[:, 'rt_private_room'] = np.where(listings_austin_subset["room_type"] == 'Private room', 1, 0)
# listings_austin_subset.loc[:, 'rt_shared_room'] = np.where(listings_austin_subset["room_type"] == 'Shared room', 1, 0)
# One-hot encode the 'room_type' column
listings_austin_subset = pd.get_dummies(listings_austin_subset, columns=['room_type'], prefix='rt', drop_first=False)

listings_austin_subset['price'].values

#Log transform the 'price' variable
listings_austin_subset.loc[:, 'log_price'] = np.log(listings_austin_subset['price'])

# Checking for missingness in the Austin subset
print('Total Records:', len(listings_austin_subset))
print("Missing values in each column:")
print(listings_austin_subset.isna().sum())

"""# **OLS Regression Analysis**"""

from pysal.model import spreg

# Defining a list of explanatory variables for the Austin listings subset
m_vars = [
    'accommodates',
    'bedrooms',
    'beds',
    'review_scores_rating',
    # 'rt_entire_home_apartment',
    # 'rt_private_room',
    # 'rt_shared_room'
]

# Running an OLS regression
ols_m = spreg.OLS(
    listings_austin_subset[["log_price"]].values,  # the dependent variable (Y)
    listings_austin_subset[m_vars].values,         # the independent variables (Xs)
    name_y='log_price',
    name_x=m_vars
)

print(ols_m.summary)

#Residual Analysis - Extract residuals
residuals = ols_m.u  # PySAL stores residuals in 'u'

#Plot histogram of residuals
plt.figure()
plt.hist(residuals, bins=30)
plt.title('Residuals Histogram (PySAL OLS)')
plt.show()

"""# **Exploring unmodeled relationships**"""

# Creating a column to store the model residuals
listings_austin_subset["ols_m_r"] = ols_m.u

# Bringing back in the neighborhood variable
listings_austin_subset = listings_austin_subset.merge(
    listings[['id', 'neighbourhood_cleansed']],
    how='left',
    on='id'
)

# Get the average value of the residual by neighborhood
mean = (
    listings_austin_subset.groupby("neighbourhood_cleansed")
    .ols_m_r.mean()
    .to_frame("neighborhood_residual")
)

# Creating a dataframe to store the residuals by neighborhood
residuals_neighborhood = listings_austin_subset.merge(
    mean,
    how="left",
    left_on="neighbourhood_cleansed",
    right_index=True
).sort_values("neighborhood_residual")

# Displaying the result
print(residuals_neighborhood.head())

import plotly.express as px
from IPython.core.display import HTML

# Step 3: Violin Plot for Residual Distribution by Neighborhood
fig = px.violin(
    residuals_neighborhood,
    x="neighbourhood_cleansed",
    y="ols_m_r",
    color="neighbourhood_cleansed",
    box=True,
    points="all"  # Show individual points
)

fig.update_layout(
    xaxis_title="Neighborhood",
    yaxis_title="Residuals",
    xaxis=dict(tickangle=90),  # Rotate labels for readability
    height=700
)

# Display Violin Plot
HTML(fig.to_html())

"""# **Neighborhoods GeoJSON**"""

import textwrap

# Producing a map of the average residual by neighborhood
import requests
import geopandas as gpd
from shapely.geometry import shape

# Reading in the NYC Neighborhoods shapefile from data file
import os

# Path to the Austin neighborhoods GeoJSON file
austin_neighborhoods_path ='/content/Austin_Neighborhoods.geojson'

# Load Austin neighborhoods GeoJSON file
austin_n = gpd.read_file(austin_neighborhoods_path)

# Display column names in the Austin neighborhoods GeoJSON file
print(austin_n.columns)

print(austin_n.head)

print(mean.columns)

print("austin_n columns:", austin_n.columns)
print("mean columns:", mean.columns)

mean = (
    listings_austin_subset.groupby("neighbourhood_cleansed")["ols_m_r"]
    .mean()
    .reset_index()
    .rename(columns={"ols_m_r": "neighborhood_residual"})
)

# Convert both columns to string type to ensure they match
austin_n['neighname'] = austin_n['neighname'].astype(str)
mean['neighbourhood_cleansed'] = mean['neighbourhood_cleansed'].astype(str)

# Now performing the merge
austin_n_r = austin_n.merge(mean, left_on='Zip_Code', right_on="neighbourhood_cleansed")

print("Unique values in austin_n['Zip_Code']:", austin_n['Zip_Code'].unique())
print("Unique values in mean['neighbourhood_cleansed']:", mean['neighbourhood_cleansed'].unique())

# Plot the average residuals by neighborhood
fig, ax = plt.subplots(1, figsize=(15, 25))

# Plot choropleth of neighborhood residuals
austin_n_r.plot(
    column='neighborhood_residual',
    cmap='vlag',
    scheme='quantiles',
    k=4,
    edgecolor='white',
    linewidth=0.1,
    alpha=0.75,
    legend=True,
    ax=ax
)

# Add basemap with OpenStreetMap provider
ctx.add_basemap(
    ax,
    crs=austin_n.crs,
    source=ctx.providers.OpenStreetMap.Mapnik
)

# Annotate each neighborhood with its zip code only
for idx, row in austin_n_r.iterrows():
    zip_code = row['Zip_Code'] if 'Zip_Code' in austin_n_r.columns else ""
    if zip_code:
        plt.annotate(
            text=zip_code,
            xy=(row.geometry.centroid.x, row.geometry.centroid.y),
            horizontalalignment='center',
            fontsize=10,
            rotation=50
        )

# Remove axis
ax.set_axis_off()

# Display plot
plt.show()

# Bringing in the geometry attribute for spatial analysis
#
residuals_neighborhood = residuals_neighborhood.merge(
    listings_austin[['id', 'geometry']], how='left', on='id'
)


# Building the spatial weights matrix using K-Nearest Neighbors (KNN) with k=5
knn = weights.KNN.from_dataframe(residuals_neighborhood, k=11)

# Constructing the spatial lag of the residuals
lag_residual = weights.spatial_lag.lag_spatial(knn, ols_m.u)

# Plotting the spatial lag of residuals against the OLS residuals
fig = px.scatter(
    x=ols_m.u.flatten(),
    y=lag_residual.flatten(),
    trendline="ols",
    width=800,
    height=800
)

# Updating the x and y axis labels
fig.update_layout(
    xaxis_title="Airbnb Residuals",
    yaxis_title="Spatially Lagged Residuals"
)

# Display the plot
fig.show()

"""# **OLS Regression Analysis w/ Spatial Atributes**

### **Spatial Feature Engineering**
"""

# Load Austin attractions data
austin_attr_path = '/content/austin_attractions.csv'
austin_attr = pd.read_csv(austin_attr_path)

# Convert to GeoDataFrame with geometry based on longitude and latitude
austin_attr_gpdf = gpd.GeoDataFrame(
    austin_attr,
    geometry=gpd.points_from_xy(austin_attr['Longitude'], austin_attr['Latitude']),
    crs="EPSG:4326"
)

# Convert the listings and attractions GeoDataFrames to a projected coordinate system for accurate distance calculations
austin_attr_gpdf_p = austin_attr_gpdf.to_crs('EPSG:2277')  # Texas Central, suitable for Austin
listings_austin_p = listings_austin.to_crs('EPSG:2277')

# Calculate the distance to each attraction for each Airbnb listing
attractions = austin_attr_gpdf['Attraction'].unique()  # Unique attraction names

# Applying a lambda function that calculates the distance between each Airbnb listing and each attraction
distances = listings_austin_p.geometry.apply(lambda g: austin_attr_gpdf_p.distance(g))

# Renaming the columns based on the attraction names
distances.columns = attractions

# Convert distances from feet to miles (assuming EPSG:2277 uses feet)
distances = distances.apply(lambda x: x / 5280, axis=1)

# Check how many attractions are within 1 to 6 miles for each listing
distances_1mi = distances.apply(lambda x: x <= 1, axis=1).sum(axis=1)
distances_2mi = distances.apply(lambda x: x <= 2, axis=1).sum(axis=1)
distances_3mi = distances.apply(lambda x: x <= 3, axis=1).sum(axis=1)
distances_4mi = distances.apply(lambda x: x <= 4, axis=1).sum(axis=1)
distances_5mi = distances.apply(lambda x: x <= 5, axis=1).sum(axis=1)
distances_6mi = distances.apply(lambda x: x <= 6, axis=1).sum(axis=1)

# Creating a DataFrame combining all distance bands
distance_df = pd.concat([distances_1mi, distances_2mi, distances_3mi, distances_4mi, distances_5mi, distances_6mi], axis=1)
distance_df.columns = ['Attr_1mi', 'Attr_2mi', 'Attr_3mi', 'Attr_4mi', 'Attr_5mi', 'Attr_6mi']

# Join the distances and distance bands back to the listings GeoDataFrame
listings_austin = listings_austin.merge(distances, left_index=True, right_index=True)
listings_austin = listings_austin.merge(distance_df, left_index=True, right_index=True)

# Displaying the first few rows of the updated listings data with distances
print(listings_austin.head())

"""**Modeling with Proximity Features**"""

# Define attraction names as a list of geographic features
g_vars = austin_attr['Attraction'].unique().tolist()

# Bring the geographic features into our subset
listings_austin_subset = listings_austin_subset.merge(
    listings_austin[['id'] + g_vars],
    how='left',
    on='id'
)

# Adding geographic features to model variables used previously
g_m_vars = m_vars + g_vars

# Display the updated variables
print("Model variables including geographic features:", g_m_vars)

# Calculate the correlation matrix for the independent variables
corr_matrix = listings_austin_subset[g_m_vars].corr()
print(corr_matrix)

import numpy as np

# Set a correlation threshold
correlation_threshold = 0.9

# Find columns with high correlation
corr_matrix = listings_austin_subset[g_m_vars].corr().abs()
upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
high_corr_features = [column for column in upper_tri.columns if any(upper_tri[column] > correlation_threshold)]

# Remove highly correlated features from g_m_vars
g_m_vars_reduced = [col for col in g_m_vars if col not in high_corr_features]
print("Removed highly correlated features:", high_corr_features)
print("Remaining features for regression:", g_m_vars_reduced)

# Running OLS regression with reduced features
ols_m_g = spreg.OLS(
    listings_austin_subset[["log_price"]].values,  # dependent variable
    listings_austin_subset[g_m_vars_reduced].values,  # independent variables without high correlations
    name_y='log_price',
    name_x=g_m_vars_reduced
)

# Print the summary of the regression results
print(ols_m_g.summary)

# Creating a DataFrame for model diagnostics, comparing OLS model without geographic features and with geographic features
model_diagnostics = pd.DataFrame(
    [[ols_m.r2, ols_m.ar2], [ols_m_g.r2, ols_m_g.ar2]],
    index=["OLS without Geographic Features", "OLS with Geographic Features"],
    columns=["R-squared", "Adjusted R-squared"]
)

# Display the diagnostics DataFrame
model_diagnostics

# Constructing the spatial lag of the residuals for the model with geographic features
lag_residual_geographic = weights.spatial_lag.lag_spatial(knn, ols_m_g.u)

# Plotting the residuals of the model with geographic features against the spatially lagged residuals
fig = px.scatter(
    x=ols_m_g.u.flatten(),
    y=lag_residual_geographic.flatten(),
    trendline="ols",
    width=800,
    height=800,
    title="Residual Plot for OLS Model with Geographic Features"
)

# Updating the x and y axis labels
fig.update_layout(
    xaxis_title="Airbnb Residuals",
    yaxis_title="Spatially Lagged Residuals"
)

# Display the plot
fig.show()

"""# **Spatial Fixed Effects Model**"""

# Bringing back in the neighbourhood_cleansed variable for spatial fixed effect analysis
listings_austin_subset = listings_austin_subset.merge(
    listings[['id', 'neighbourhood_cleansed']],
    how='left',
    on='id'
)

# Check the columns in listings_austin_subset to verify the available columns
print("Columns in listings_austin_subset:", listings_austin_subset.columns)

# Spatial fixed effect model implementation with corrected neighborhood column
sfe_m = spreg.OLS_Regimes(
    # The dependent variable (Y) - Log Price (log_price)
    listings_austin_subset[["log_price"]].values,

    # The independent variables (Xs)
    listings_austin_subset[g_m_vars_reduced].values,  # Use reduced set of geographic features

    # Variable specifying which neighborhood each Airbnb falls within
    listings_austin_subset["neighbourhood_cleansed_y"].tolist(),

    # Vary constant by each cross-section/group
    constant_regi="many",

    # Specify which variables are kept constant by group
    cols2regi=[False] * len(g_m_vars_reduced),

    # Tell the model to estimate a single sigma coefficient
    regime_err_sep=False,

    # Dependent variable name
    name_y="log_price",

    # Independent variables names
    name_x=g_m_vars_reduced,
)

# Printing the model summary
print(sfe_m.summary)

# Print the Chow statistic for the spatial fixed effects model
print("Chow Statistic (Joint):", sfe_m.chow.joint)

# Constructing the spatial lag of the residuals for the spatial fixed effects model
lag_residual_sfe = weights.spatial_lag.lag_spatial(knn, sfe_m.u)

# Plotting the residuals of the spatial fixed effects model against the spatially lagged residuals
fig = px.scatter(
    x=sfe_m.u.flatten(),
    y=lag_residual_sfe.flatten(),
    trendline="ols",
    width=800,
    height=800,
    title="Residual Plot for Spatial Fixed Effects (SFE) OLS Model"
)

# Updating the x and y axis labels
fig.update_layout(
    xaxis_title="SFE Model Residuals",
    yaxis_title="Spatially Lagged Residuals"
)

# Display the plot
fig.show()

"""# **Geographically Weighted Regression**"""

from mgwr.gwr import GWR
from mgwr.sel_bw import Sel_BW
import joblib

# # Encoding categorical variables

# # Room Type encoding, assuming all 0s represent 'Hotel room'
# listings_austin['rt_entire_home_apartment'] = np.where(listings_austin["room_type"] == 'Entire home/apt', 1, 0)
# listings_austin['rt_private_room'] = np.where(listings_austin["room_type"] == 'Private room', 1, 0)
# listings_austin['rt_shared_room'] = np.where(listings_austin["room_type"] == 'Shared room', 1, 0)

# Cleaning up the price column
listings_austin['price'] = listings_austin['price'].astype(str).str.replace('$', '').str.replace(',', '').astype(float)

# Logging the price variable
listings_austin['log_price'] = np.log(listings_austin['price'])

# # Removing rows with missing values in key columns
# listings_austin = listings_austin[listings_austin['bedrooms'].notna()]
# listings_austin = listings_austin[listings_austin['review_scores_rating'].notna()]
# listings_austin = listings_austin[listings_austin['beds'].notna()]

# Check for any remaining missing values
print(listings_austin.isna().sum())

# Reproject coordinates (if needed)
from pyproj import Proj, transform
input_proj = Proj(init='epsg:4326')
output_proj = Proj(init='epsg:32614')
listings_austin['proj_x'], listings_austin['proj_y'] = transform(
    input_proj, output_proj, listings_austin.geometry.x, listings_austin.geometry.y
)
coords = list(zip(listings_austin['proj_x'], listings_austin['proj_y']))

# Select explanatory variables and dependent variable
exp_vars = listings_austin[['accommodates', 'bedrooms', 'beds', 'review_scores_rating']].values
y = listings_austin['log_price'].values.reshape((-1, 1))

!pip install --upgrade mgwr

from mgwr.sel_bw import Sel_BW

# Selecting the optimal bandwidth for the explanatory variables
gwr_selector = Sel_BW(coords, y, exp_vars, spherical=False)
gwr_bw = gwr_selector.search(bw_min=50)

# Displaying the optimal bandwidth
gwr_bw

from mgwr.gwr import GWR

# Fit the GWR model to your data
gwr_results = GWR(coords, y, exp_vars, gwr_bw).fit()

# Print the results of the GWR model
print(gwr_results.summary())

"""The GWR model is performing good here. The GWR Model's Adjusted R-squared of 0.683 demonstrates its effectiveness in improving the explanatory power compared to traditional OLS models with fixed effects."""

import joblib
joblib.dump(gwr_results, 'gwr_model.pkl')
joblib.dump(gwr_bw, 'gwr_bw.pkl')
joblib.dump(listings_austin, 'gwr_data.pkl')

residuals = gwr_results.resid_response

# Constructing the spatial lag of the GWR residuals
lag_residual_gwr = weights.spatial_lag.lag_spatial(knn, residuals)

# Plotting the GWR residuals against their spatially lagged values
fig = px.scatter(
    x=residuals.flatten(),
    y=lag_residual_gwr.flatten(),
    trendline="ols",
    width=800,
    height=800,
    title="Residual Plot for GWR Model"
)

# Updating the x and y axis labels
fig.update_layout(
    xaxis_title="GWR Model Residuals",
    yaxis_title="Spatially Lagged Residuals"
)

# Display the plot
fig.show()

"""# **Multi-scale Geographically Weighted Regression**"""

# Importing MGWR
from mgwr.gwr import MGWR


listings_austin_sample = listings_austin.copy()

exp_vars = listings_austin_sample[['accommodates', 'bedrooms', 'beds', 'review_scores_rating']].values
y = listings_austin_sample['log_price'].values.reshape((-1, 1))
coords = list(zip(listings_austin_sample.geometry.x, listings_austin_sample.geometry.y))

# Standardizing the explanatory variables and dependent variable
exp_vars = (exp_vars - np.mean(exp_vars, axis=0)) / np.std(exp_vars, axis=0)
y = (y - np.mean(y, axis=0)) / np.std(y, axis=0)

# Calculate the correlation matrix to check for multicollinearity
corr_matrix = pd.DataFrame(exp_vars).corr()
print(corr_matrix)

from statsmodels.stats.outliers_influence import variance_inflation_factor

# Compute VIF for each explanatory variable
vif_data = pd.DataFrame()
vif_data["feature"] = ['accommodates', 'bedrooms', 'beds', 'review_scores_rating']
vif_data["VIF"] = [variance_inflation_factor(exp_vars, i) for i in range(exp_vars.shape[1])]

print(vif_data)

# Step 1: Sample 10% of the data
sample_data = listings_austin.sample(frac=0.2, random_state=42)  # Use 10% of data

# Step 2: Define coordinates, y (dependent variable), and exp_vars (explanatory variables)
coords = list(zip(sample_data.geometry.x, sample_data.geometry.y))
y = sample_data['log_price'].values.reshape((-1, 1))
exp_vars = sample_data[['accommodates', 'bedrooms', 'beds', 'review_scores_rating']].values

# Step 3: Run selector search with either a higher multi_bw_min or max_iters limit

# Option A: Use a higher minimum bandwidth for faster search
selector = Sel_BW(coords, y, exp_vars, multi=True, spherical=True)
optimal_bandwidth = selector.search(multi_bw_min=[50])  # Change multi_bw_min to a higher value

# Print the optimal bandwidth
print("Optimal Bandwidth:", optimal_bandwidth)

from mgwr.gwr import MGWR

# Step 1: Define the MGWR model with the optimal bandwidth selector
mgwr_model = MGWR(coords, y, exp_vars, selector, sigma2_v1=True)

# Step 2: Fit the MGWR model
mgwr_results = mgwr_model.fit()

# Step 3: Print the summary of the MGWR model results
print(mgwr_results.summary())

"""# **K means Clustering**"""

from mgwr.gwr import GWR
from mgwr.sel_bw import Sel_BW
import joblib
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns
import contextily as ctx
import geopandas as gpd

# One-hot encoding for room type (including Hotel room)
room_type_encoded = pd.get_dummies(listings_austin['room_type'], prefix='room_type')
listings_austin = pd.concat([listings_austin, room_type_encoded], axis=1)

# Ensure the price column is properly formatted
listings_austin['price'] = listings_austin['price'].astype(str).str.replace('$', '').str.replace(',', '').astype(float)
listings_austin['log_price'] = np.log(listings_austin['price'])

# Drop rows with missing values in critical columns
listings_austin = listings_austin.dropna(subset=['bedrooms', 'review_scores_rating', 'beds'])

# Check for remaining missing values
print(listings_austin.isna().sum())

# K-means Clustering Implementation
# Select relevant features for clustering (including room type encoding)
X = listings_austin[['latitude', 'longitude', 'price', 'room_type_Entire home/apt', 'room_type_Private room', 'room_type_Shared room', 'room_type_Hotel room']]

# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Step 1: Determine Optimal Number of Clusters Using the Elbow Method
inertia = []
K_range = range(1, 11)  # Test for cluster counts from 1 to 10

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X_scaled)
    inertia.append(kmeans.inertia_)

# Step 2: Plot the Elbow Method Results
plt.figure(figsize=(8, 5))
plt.plot(K_range, inertia, marker='o')
plt.xlabel('Number of clusters')
plt.ylabel('Inertia')
plt.title('Elbow Method for Optimal Clusters')
plt.grid(True)
plt.show()

# Apply K-means clustering
kmeans = KMeans(n_clusters=4, random_state=42)
listings_austin['cluster'] = kmeans.fit_predict(X_scaled)

# Visualize clusters on a map
fig, ax = plt.subplots(figsize=(12, 10))
listings_austin.plot(ax=ax, column='cluster', cmap='viridis', markersize=5, legend=True)
ctx.add_basemap(ax, crs=listings_austin.crs.to_string())
plt.title('K-means Clustering of Airbnb Listings')
plt.show()

# Plot clusters with attractions
fig, ax = plt.subplots(figsize=(12, 10))

# Plot Airbnb listings by cluster
listings_austin.plot(ax=ax, column='cluster', cmap='viridis', markersize=5, legend=True)

# Plot attractions on top of clusters
attractions_gpdf.plot(ax=ax, color='red', markersize=20, label='Attractions', alpha=0.8)

# Add basemap for context
ctx.add_basemap(ax, crs=listings_austin.crs.to_string(), source=ctx.providers.OpenStreetMap.Mapnik)

# Customize plot appearance
plt.title('K-means Clustering of Airbnb Listings with Attractions')
plt.legend()
plt.show()

#Calculate Distances and Visualize Results
from shapely.ops import nearest_points
from shapely.geometry import Point
import geopandas as gpd
import matplotlib.pyplot as plt
import seaborn as sns

# Ensure CRS is consistent between Airbnb listings and attractions
attractions_gpdf = attractions_gpdf.to_crs(listings_austin.crs)

# Function to calculate distance to nearest attraction
def nearest_attraction_distance(point, attractions_gpdf):
    nearest_geom = nearest_points(point, attractions_gpdf.unary_union)[1]
    return point.distance(nearest_geom)

# Calculate distance to nearest attraction for each listing
listings_austin['distance_to_attraction'] = listings_austin['geometry'].apply(
    lambda x: nearest_attraction_distance(x, attractions_gpdf)
)

# Visualize relationship between price and distance to attraction by cluster
plt.figure(figsize=(12, 8))
sns.scatterplot(
    x='distance_to_attraction',
    y='price',
    hue='cluster',
    data=listings_austin,
    palette='viridis'
)
plt.title('Price vs Distance to Nearest Attraction (Colored by Cluster)')
plt.xlabel('Distance to Nearest Attraction (meters)')
plt.ylabel('Price (USD)')
plt.legend(title='Cluster')
plt.show()

# Display summary statistics for each cluster
summary = listings_austin.groupby('cluster')['distance_to_attraction'].describe()
print(summary)

#Regression Analysis
from sklearn.linear_model import LinearRegression
import numpy as np

# Perform regression for each cluster
plt.figure(figsize=(12, 8))

for cluster in listings_austin['cluster'].unique():
    cluster_data = listings_austin[listings_austin['cluster'] == cluster]

    # Prepare data
    X = cluster_data['distance_to_attraction'].values.reshape(-1, 1)
    y = cluster_data['price'].values

    # Fit regression model
    model = LinearRegression()
    model.fit(X, y)

    # Predict and plot regression line
    y_pred = model.predict(X)
    plt.scatter(X, y, label=f'Cluster {cluster}', alpha=0.6)
    plt.plot(X, y_pred, label=f'Regression Line - Cluster {cluster}', linewidth=2)

    # Print regression coefficients
    print(f"Cluster {cluster}: Coefficient = {model.coef_[0]:.2f}, Intercept = {model.intercept_:.2f}")

# Plot settings
plt.title('Regression of Price vs Distance to Nearest Attraction by Cluster')
plt.xlabel('Distance to Nearest Attraction (meters)')
plt.ylabel('Price (USD)')
plt.legend()
plt.show()

#Residual Analysis and Cross-Validation
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error, r2_score

# Perform regression and residual analysis for each cluster
plt.figure(figsize=(14, 10))

for cluster in listings_austin['cluster'].unique():
    cluster_data = listings_austin[listings_austin['cluster'] == cluster]

    # Prepare data
    X = cluster_data['distance_to_attraction'].values.reshape(-1, 1)
    y = cluster_data['price'].values

    # Fit regression model
    model = LinearRegression()
    model.fit(X, y)

    # Predict and calculate residuals
    y_pred = model.predict(X)
    residuals = y - y_pred

    # Plot residuals
    plt.scatter(X, residuals, label=f'Cluster {cluster}', alpha=0.6)

    # Print regression coefficients and model performance metrics
    print(f"Cluster {cluster}: Coefficient = {model.coef_[0]:.2f}, Intercept = {model.intercept_:.2f}")
    print(f"  Mean Squared Error (MSE): {mean_squared_error(y, y_pred):.2f}")
    print(f"  R-squared (R²): {r2_score(y, y_pred):.2f}")

    # Cross-validation (5-fold)
    scores = cross_val_score(model, X, y, cv=5, scoring='r2')
    print(f"  Cross-validated R²: {scores.mean():.2f} ± {scores.std():.2f}\n")

# Plot settings for residuals
plt.axhline(y=0, color='red', linestyle='--')
plt.title('Residual Plot for Price vs Distance to Nearest Attraction by Cluster')
plt.xlabel('Distance to Nearest Attraction (meters)')
plt.ylabel('Residuals (Price - Predicted Price)')
plt.legend()
plt.show()

# Calculate mean values of clustering features within each cluster
cluster_summary = listings_austin.groupby('cluster')[['latitude', 'longitude', 'price',
                                                      'room_type_Entire home/apt',
                                                      'room_type_Private room',
                                                      'room_type_Shared room',
                                                      'room_type_Hotel room']].mean()

# Display the cluster summary as a table
print("Cluster Summary (Mean Values):")
print(cluster_summary)

# Visualize the cluster summary using bar plots
plt.figure(figsize=(12, 6))
cluster_summary.plot(kind='bar', figsize=(10, 6), legend=True)
plt.title('Mean Values of Clustering Features by Cluster')
plt.ylabel('Mean Value')
plt.xlabel('Cluster')
plt.xticks(rotation=0)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

# Calculate mean values of clustering features within each cluster
cluster_summary = listings_austin.groupby('cluster')[['latitude', 'longitude', 'price',
                                                      'room_type_Entire home/apt',
                                                      'room_type_Private room',
                                                      'room_type_Shared room',
                                                      'room_type_Hotel room']].mean()

# Count the number of listings per cluster
cluster_counts = listings_austin['cluster'].value_counts().sort_index()

# Add the counts to the summary
cluster_summary['count'] = cluster_counts

# Display the cluster summary as a table
print("Cluster Summary (Mean Values and Counts):")
print(cluster_summary)

# Visualize the cluster summary using bar plots
plt.figure(figsize=(14, 7))
cluster_summary.drop(columns=['latitude', 'longitude']).plot(kind='bar', figsize=(12, 6), legend=True)
plt.title('Mean Values of Clustering Features by Cluster')
plt.ylabel('Mean Value')
plt.xlabel('Cluster')
plt.xticks(rotation=0)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

# Plot cluster counts separately
plt.figure(figsize=(10, 6))
cluster_counts.plot(kind='bar', color='skyblue')
plt.title('Number of Listings per Cluster')
plt.ylabel('Count')
plt.xlabel('Cluster')
plt.xticks(rotation=0)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()